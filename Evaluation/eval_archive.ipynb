{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoX could not be found!\n",
      "\n",
      "    If you do not have SoX, proceed here:\n",
      "     - - - http://sox.sourceforge.net/ - - -\n",
      "\n",
      "    If you do (or think that you should) have SoX, double-check your\n",
      "    path variables.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Python\n",
    "import os\n",
    "import csv\n",
    "# Third party\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchaudio\n",
    "from torchaudio.transforms import Fade\n",
    "\n",
    "import nussl\n",
    "from nussl.ml import SeparationModel\n",
    "\n",
    "from mir_eval import separation\n",
    "\n",
    "# Self\n",
    "from models.UNet import UNetSpect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_dirs(musdb18_directory):\n",
    "    \"\"\"Takes in a MUSDB18 directory and returns a list of strings of all the folders in the directory.\n",
    "    If MUSDB18 is broken down into subfolders (e.g. test, train, and validation), the list of directories will go one layer deep.\n",
    "    Useful for looping MUSDB18/<song_name>/mixture.wav or MUSDB18/test/<song_name>/mixture.wav.\n",
    "\n",
    "    Args:\n",
    "        directory (str): path to MUSDB18 dataset.\n",
    "\n",
    "    Returns:\n",
    "        list_of_dirs (list): A list of strings containing the relative path of each song in MUSDB18.\n",
    "    \"\"\"\n",
    "    list_of_dirs = []\n",
    "    if 'train' in os.listdir(musdb18_directory):\n",
    "        for sub_folder in os.listdir(musdb18_directory):\n",
    "            for song_name in os.listdir(musdb18_directory + sub_folder):\n",
    "                if song_name not in ['bass','drums','vocals','other']:\n",
    "                    list_of_dirs.append(musdb18_directory + sub_folder + '/' + song_name +'/')\n",
    "    else:\n",
    "        for song_name in os.listdir(musdb18_directory):\n",
    "            list_of_dirs.append(musdb18_directory + song_name +'/')\n",
    "\n",
    "    return list_of_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_device(waveform):\n",
    "    '''Moves waveform to cpu for bss_eval_sources.  Both waveforms need to be on the same device.\n",
    "    Args:\n",
    "        waveform (torch.Tensor): A torch.Tensor waveform.\n",
    "\n",
    "    Returns:\n",
    "        waveform (torch.Tensor): A torch.Tensor waveform.\n",
    "    '''\n",
    "    if waveform.get_device() == 0:\n",
    "        waveform = waveform.cpu()\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bss_eval(reference_source, estimated_source):\n",
    "    \"\"\"Computes SDR, SIR, and SAR for a reference source and an estimated source.\n",
    "\n",
    "    Args:\n",
    "        reference_source (np.ndarray): A np.ndarray of the reference source.\n",
    "        estimated_source (np.ndarray):  A np.ndarray of the estimated source.\n",
    "\n",
    "    Returns:\n",
    "        (sdr,sir,sar) (tuple): A tuple containing sdr, sir, and sar.\n",
    "    \"\"\"\n",
    "    _eval = separation.bss_eval_sources(reference_source, estimated_source)\n",
    "    sdr = _eval[0].mean()\n",
    "    sir = _eval[1].mean()\n",
    "    sar = _eval[2].mean()\n",
    "    return (sdr, sir, sar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_sources(\n",
    "        model,\n",
    "        mix,\n",
    "        segment=10.,\n",
    "        overlap=0.1,\n",
    "        device=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply model to a given mixture. Use fade, and add segments together in order to add model segment by segment.\n",
    "\n",
    "    Args:\n",
    "        segment (int): segment length in seconds\n",
    "        device (torch.device, str, or None): if provided, device on which to\n",
    "            execute the computation, otherwise `mix.device` is assumed.\n",
    "            When `device` is different from `mix.device`, only local computations will\n",
    "            be on `device`, while the entire tracks will be stored on `mix.device`.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = mix.device\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "\n",
    "    batch, channels, length = mix.shape\n",
    "\n",
    "    chunk_len = int(sample_rate * segment * (1 + overlap))\n",
    "    start = 0\n",
    "    end = chunk_len\n",
    "    overlap_frames = overlap * sample_rate\n",
    "    fade = Fade(fade_in_len=0, fade_out_len=int(overlap_frames), fade_shape='linear')\n",
    "\n",
    "    final = torch.zeros(batch, len(model.sources), channels, length, device=device)\n",
    "\n",
    "    while start < length - overlap_frames:\n",
    "        chunk = mix[:, :, start:end]\n",
    "        with torch.no_grad():\n",
    "            out = model.forward(chunk)\n",
    "        out = fade(out)\n",
    "        final[:, :, :, start:end] += out\n",
    "        if start == 0:\n",
    "            fade.fade_in_len = int(overlap_frames)\n",
    "            start += int(chunk_len - overlap_frames)\n",
    "        else:\n",
    "            start += chunk_len\n",
    "        end += chunk_len\n",
    "        if end >= length:\n",
    "            fade.fade_out_len = 0\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,list_of_mixtures,segment=10.,overlap=0.1,device=None,):\n",
    "    model_name = model.__class__.__name__\n",
    "    print(model_name)\n",
    "\n",
    "    \n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    print(device)\n",
    "    for idx, _dir in enumerate(list_of_mixtures):\n",
    "        print(idx, _dir)\n",
    "        reference_mixture, reference_sample_rate = torchaudio.load(_dir + 'mixture.wav')\n",
    "        reference_mixture = reference_mixture.to(device)\n",
    "        \n",
    "        reference_mixture_mean = reference_mixture.mean(0)\n",
    "        reference_mixture = (reference_mixture - reference_mixture_mean.mean()) / reference_mixture_mean.std()  # normalization\n",
    "        \n",
    "        sources = separate_sources(\n",
    "            model,\n",
    "            reference_mixture[None],\n",
    "            device=device,\n",
    "            segment=segment,\n",
    "            overlap=overlap,\n",
    "            )[0]\n",
    "        sources = sources * reference_mixture_mean.std() + reference_mixture_mean.mean()\n",
    "        sources_list = model.sources\n",
    "        sources = list(sources)\n",
    "        audios = dict(zip(sources_list, sources))\n",
    "\n",
    "        for source in sources_list:\n",
    "            print(source)\n",
    "\n",
    "            original_mixture, original_sample_rate = torchaudio.load(_dir + source + '.wav')\n",
    "            original_mixture = original_mixture.to(device)\n",
    "            sdr, sir, sar = bss_eval(same_device(audios[source]),same_device(original_mixture))\n",
    "            print(sdr, sir, sar)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['drums', 'bass', 'other', 'vocals']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = HDEMUCS_HIGH_MUSDB_PLUS\n",
    "model = bundle.get_model()\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "# print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDemucs\n",
      "cpu\n",
      "0 data/MUSDB18HQ/test/Al James - Schoolboy Facination/\n",
      "drums\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sum() received an invalid combination of arguments - got (axis=tuple, out=NoneType, ), but expected one of:\n * (*, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: axis, out\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[177], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m list_of_dirs \u001b[39m=\u001b[39m get_source_dirs(\u001b[39m'\u001b[39m\u001b[39mdata/MUSDB18HQ/\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m evaluate_model(model,list_of_dirs[\u001b[39m0\u001b[39;49m:\u001b[39m1\u001b[39;49m],segment\u001b[39m=\u001b[39;49m\u001b[39m10.\u001b[39;49m,overlap\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,device\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[175], line 38\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, list_of_mixtures, segment, overlap, device)\u001b[0m\n\u001b[0;32m     36\u001b[0m         original_mixture, original_sample_rate \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39mload(_dir \u001b[39m+\u001b[39m source \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.wav\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     37\u001b[0m         original_mixture \u001b[39m=\u001b[39m original_mixture\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 38\u001b[0m         sdr, sir, sar \u001b[39m=\u001b[39m bss_eval(same_device(audios[source]),same_device(original_mixture))\n\u001b[0;32m     39\u001b[0m         \u001b[39mprint\u001b[39m(sdr, sir, sar)\n\u001b[0;32m     44\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[122], line 11\u001b[0m, in \u001b[0;36mbss_eval\u001b[1;34m(reference_source, estimated_source)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbss_eval\u001b[39m(reference_source, estimated_source):\n\u001b[0;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Computes SDR, SIR, and SAR for a reference source and an estimated source.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39m        (sdr,sir,sar) (tuple): A tuple containing sdr, sir, and sar.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     _eval \u001b[39m=\u001b[39m separation\u001b[39m.\u001b[39;49mbss_eval_sources(reference_source, estimated_source)\n\u001b[0;32m     12\u001b[0m     sdr \u001b[39m=\u001b[39m _eval[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmean()\n\u001b[0;32m     13\u001b[0m     sir \u001b[39m=\u001b[39m _eval[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\Users\\ja238f\\Anaconda3\\envs\\at\\lib\\site-packages\\mir_eval\\separation.py:194\u001b[0m, in \u001b[0;36mbss_eval_sources\u001b[1;34m(reference_sources, estimated_sources, compute_permutation)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[39mif\u001b[39;00m reference_sources\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    192\u001b[0m     reference_sources \u001b[39m=\u001b[39m reference_sources[np\u001b[39m.\u001b[39mnewaxis, :]\n\u001b[1;32m--> 194\u001b[0m validate(reference_sources, estimated_sources)\n\u001b[0;32m    195\u001b[0m \u001b[39m# If empty matrices were supplied, return empty lists (special case)\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39mif\u001b[39;00m reference_sources\u001b[39m.\u001b[39msize \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m estimated_sources\u001b[39m.\u001b[39msize \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ja238f\\Anaconda3\\envs\\at\\lib\\site-packages\\mir_eval\\separation.py:93\u001b[0m, in \u001b[0;36mvalidate\u001b[1;34m(reference_sources, estimated_sources)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[39mif\u001b[39;00m reference_sources\u001b[39m.\u001b[39msize \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     90\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mreference_sources is empty, should be of size \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     91\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39m(nsrc, nsample).  sdr, sir, sar, and perm will all \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mbe empty np.ndarrays\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[39melif\u001b[39;00m _any_source_silent(reference_sources):\n\u001b[0;32m     94\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAll the reference sources should be non-silent (not \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     95\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mall-zeros), but at least one of the reference \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     96\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39msources is all 0s, which introduces ambiguity to the\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     97\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39m evaluation. (Otherwise we can add infinitely many \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     98\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mall-zero sources.)\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    100\u001b[0m \u001b[39mif\u001b[39;00m estimated_sources\u001b[39m.\u001b[39msize \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ja238f\\Anaconda3\\envs\\at\\lib\\site-packages\\mir_eval\\separation.py:126\u001b[0m, in \u001b[0;36m_any_source_silent\u001b[1;34m(sources)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_any_source_silent\u001b[39m(sources):\n\u001b[0;32m    125\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns true if the parameter sources has any silent first dimensions\"\"\"\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39many(np\u001b[39m.\u001b[39mall(np\u001b[39m.\u001b[39;49msum(\n\u001b[0;32m    127\u001b[0m         sources, axis\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(\u001b[39mrange\u001b[39;49m(\u001b[39m2\u001b[39;49m, sources\u001b[39m.\u001b[39;49mndim))) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msum\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ja238f\\Anaconda3\\envs\\at\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2298\u001b[0m, in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2295\u001b[0m         \u001b[39mreturn\u001b[39;00m out\n\u001b[0;32m   2296\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n\u001b[1;32m-> 2298\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapreduction(a, np\u001b[39m.\u001b[39;49madd, \u001b[39m'\u001b[39;49m\u001b[39msum\u001b[39;49m\u001b[39m'\u001b[39;49m, axis, dtype, out, keepdims\u001b[39m=\u001b[39;49mkeepdims,\n\u001b[0;32m   2299\u001b[0m                       initial\u001b[39m=\u001b[39;49minitial, where\u001b[39m=\u001b[39;49mwhere)\n",
      "File \u001b[1;32mc:\\Users\\ja238f\\Anaconda3\\envs\\at\\lib\\site-packages\\numpy\\core\\fromnumeric.py:84\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n\u001b[0;32m     83\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n\u001b[0;32m     86\u001b[0m \u001b[39mreturn\u001b[39;00m ufunc\u001b[39m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: sum() received an invalid combination of arguments - got (axis=tuple, out=NoneType, ), but expected one of:\n * (*, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: axis, out\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n"
     ]
    }
   ],
   "source": [
    "list_of_dirs = get_source_dirs('data/MUSDB18HQ/')\n",
    "evaluate_model(model,list_of_dirs[0:1],segment=10.,overlap=0.1,device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixture = 'data/MUSDB18HQ/train/A Classic Education - NightOwl/mixture.wav'\n",
    "# waveform, sample_rate = torchaudio.load(mixture)  # replace SAMPLE_SONG with desired path for different song\n",
    "# waveform = waveform.to(device)\n",
    "\n",
    "# ref = waveform.mean(0)\n",
    "# waveform = (waveform - ref.mean()) / ref.std()  # normalization\n",
    "# model(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = UNetSpect().build()\n",
    "# checkpoint = torch.load(\"checkpoints/best.model.pth\")\n",
    "# model = SeparationModel(checkpoint[\"config\"])\n",
    "# model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "# model(audio_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixture = 'data/MUSDB18HQ/train/A Classic Education - NightOwl\\mixture.wav'\n",
    "# waveform, sample_rate = torchaudio.load(mixture)  # replace SAMPLE_SONG with desired path for different song\n",
    "# waveform = waveform.to(device)\n",
    "\n",
    "# ref = waveform.mean(0)\n",
    "# waveform = (waveform - ref.mean()) / ref.std()  # normalization\n",
    "\n",
    "\n",
    "# separate_sources(model,\n",
    "#         waveform[None],\n",
    "#         segment=10,\n",
    "#         overlap=0.1,\n",
    "#         device=None,\n",
    "#         sample_rate=sample_rate\n",
    "# )\n",
    "\n",
    "# sources = sources * ref.std() + ref.mean()\n",
    "\n",
    "# sources_list = model.sources\n",
    "# sources = list(sources)\n",
    "\n",
    "# audios = dict(zip(sources_list, sources))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "at",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
